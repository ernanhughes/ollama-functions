{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating embedding for: doc1\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating embedding for: doc2\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Searching for: Tell me about AI tools.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[('doc1', np.float64(0.6860680795567213))]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OllamaEmbeddingSearch:\n",
    "    def __init__(self, model=\"nomic-embed-text\"):\n",
    "        self.model = model\n",
    "        self.data_store = {}\n",
    "\n",
    "    def add_data(self, identifier, text):\n",
    "        \"\"\"Generates an embedding for the given text and stores it.\"\"\"\n",
    "        logger.info(f\"Generating embedding for: {identifier}\")\n",
    "        try:\n",
    "            embedding = np.array(ollama.embed(model=self.model, input=text)[\"embeddings\"]).squeeze()\n",
    "            self.data_store[identifier] = embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embedding: {e}\")\n",
    "\n",
    "    def search(self, query, top_k=1):\n",
    "        \"\"\"Finds the most similar text in the stored data based on cosine similarity.\"\"\"\n",
    "        logger.info(f\"Searching for: {query}\")\n",
    "        try:\n",
    "            query_embedding = np.array(ollama.embed(model=self.model, input=query)[\"embeddings\"]).squeeze()\n",
    "\n",
    "            similarities = {}\n",
    "            for identifier, stored_embedding in self.data_store.items():\n",
    "                similarity = np.dot(query_embedding, stored_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(stored_embedding))\n",
    "                similarities[identifier] = similarity\n",
    "            \n",
    "            sorted_results = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "            return sorted_results[:top_k]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in search: {e}\")\n",
    "            return []\n",
    "\n",
    "# Example Usage\n",
    "embedding_search = OllamaEmbeddingSearch()\n",
    "embedding_search.add_data(\"doc1\", \"Ollama is a powerful AI tool.\")\n",
    "embedding_search.add_data(\"doc2\", \"Machine learning models can be used for text processing.\")\n",
    "\n",
    "query = \"Tell me about AI tools.\"\n",
    "logger.info(embedding_search.search(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "\n",
    "# Simulates an API call to get flight times\n",
    "# In a real application, this would fetch data from a live database or API\n",
    "def get_flight_times(departure: str, arrival: str) -> str:\n",
    "    flights = {\n",
    "        \"NYC-LAX\": {\n",
    "            \"departure\": \"08:00 AM\",\n",
    "            \"arrival\": \"11:30 AM\",\n",
    "            \"duration\": \"5h 30m\",\n",
    "        },\n",
    "        \"LAX-NYC\": {\n",
    "            \"departure\": \"02:00 PM\",\n",
    "            \"arrival\": \"10:30 PM\",\n",
    "            \"duration\": \"5h 30m\",\n",
    "        },\n",
    "        \"LHR-JFK\": {\n",
    "            \"departure\": \"10:00 AM\",\n",
    "            \"arrival\": \"01:00 PM\",\n",
    "            \"duration\": \"8h 00m\",\n",
    "        },\n",
    "        \"JFK-LHR\": {\n",
    "            \"departure\": \"09:00 PM\",\n",
    "            \"arrival\": \"09:00 AM\",\n",
    "            \"duration\": \"7h 00m\",\n",
    "        },\n",
    "        \"CDG-DXB\": {\n",
    "            \"departure\": \"11:00 AM\",\n",
    "            \"arrival\": \"08:00 PM\",\n",
    "            \"duration\": \"6h 00m\",\n",
    "        },\n",
    "        \"DXB-CDG\": {\n",
    "            \"departure\": \"03:00 AM\",\n",
    "            \"arrival\": \"07:30 AM\",\n",
    "            \"duration\": \"7h 30m\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    key = f\"{departure}-{arrival}\".upper()\n",
    "    return json.dumps(flights.get(key, {\"error\": \"Flight not found\"}))\n",
    "\n",
    "\n",
    "\n",
    "# Search data related to Artificial Intelligence in a vector database\n",
    "def search_data_in_vector_db(query: str) -> str:\n",
    "    query_vectors = embedding_fn.encode_queries([query])\n",
    "\n",
    "    res = client.search(\n",
    "        collection_name=\"demo_collection\",\n",
    "        data=query_vectors,\n",
    "        limit=2,\n",
    "        output_fields=[\"text\", \"subject\"],  # specifies fields to be returned\n",
    "    )\n",
    "    print(res)\n",
    "    return json.dumps(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model: str, question: str):\n",
    "    client = ollama.Client()\n",
    "    # Initialize conversation with a user query\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "    # First API call: Send the query and function description to the model\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_flight_times\",\n",
    "                    \"description\": \"Get the flight times between two cities\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"departure\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The departure city (airport code)\",\n",
    "                            },\n",
    "                            \"arrival\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The arrival city (airport code)\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"departure\", \"arrival\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"search_data_in_vector_db\",\n",
    "                    \"description\": \"Search about Artificial Intelligence data in a vector database\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The search query\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"query\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Add the model's response to the conversation history\n",
    "    messages.append(response[\"message\"])\n",
    "\n",
    "    # Check if the model decided to use the provided function\n",
    "    if not response[\"message\"].get(\"tool_calls\"):\n",
    "        print(\"The model didn't use the function. Its response was:\")\n",
    "        print(response[\"message\"][\"content\"])\n",
    "        return\n",
    "\n",
    "    # Process function calls made by the model\n",
    "    if response[\"message\"].get(\"tool_calls\"):\n",
    "        available_functions = {\n",
    "            \"get_flight_times\": get_flight_times,\n",
    "            \"search_data_in_vector_db\": search_data_in_vector_db,\n",
    "        }\n",
    "        for tool in response[\"message\"][\"tool_calls\"]:\n",
    "            function_to_call = available_functions[tool[\"function\"][\"name\"]]\n",
    "            function_args = tool[\"function\"][\"arguments\"]\n",
    "            function_response = function_to_call(**function_args)\n",
    "            # Add function response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = client.chat(model=model, messages=messages)\n",
    "    print(final_response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flight time from New York (NYC) to Los Angeles (LAX) can vary depending on several factors such as the airline, route, and weather conditions. However, on average, a direct flight from New York to Los Angeles takes around 5 hours and 30 minutes to 6 hours.\n",
      "\n",
      "It's worth noting that there are no direct flights from New York to Los Angeles. Most flights have a layover or connection in another city before reaching their destination. The total travel time, including check-in, security screening, boarding, and any connections, can range from 8 to 12 hours or more, depending on the specific flight schedule and airline.\n",
      "\n",
      "To get a more accurate estimate of flight times, I recommend checking with airlines or flight search engines such as Google Flights, Skyscanner, or Kayak.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the flight time from New York (NYC) to Los Angeles (LAX)?\"\n",
    "run(\"llama3.2\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhen was Artificial Intelligence founded?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama3.2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(model, question)\u001b[39m\n\u001b[32m     68\u001b[39m function_to_call = available_functions[tool[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     69\u001b[39m function_args = tool[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33marguments\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m function_response = \u001b[43mfunction_to_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunction_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Add function response to the conversation\u001b[39;00m\n\u001b[32m     72\u001b[39m messages.append(\n\u001b[32m     73\u001b[39m     {\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtool\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: function_response,\n\u001b[32m     76\u001b[39m     }\n\u001b[32m     77\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36msearch_data_in_vector_db\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_data_in_vector_db\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     query_vectors = \u001b[43membedding_fn\u001b[49m.encode_queries([query])\n\u001b[32m     50\u001b[39m     res = client.search(\n\u001b[32m     51\u001b[39m         collection_name=\u001b[33m\"\u001b[39m\u001b[33mdemo_collection\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m         data=query_vectors,\n\u001b[32m     53\u001b[39m         limit=\u001b[32m2\u001b[39m,\n\u001b[32m     54\u001b[39m         output_fields=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# specifies fields to be returned\u001b[39;00m\n\u001b[32m     55\u001b[39m     )\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_fn' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"When was Artificial Intelligence founded?\"\n",
    "run(\"llama3.2\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "response = ollama.generate(\"Add 5 and 7.\", functions=[add_numbers])\n",
    "print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
